{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30261,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2022-11-27T18:30:43.927939Z","iopub.execute_input":"2022-11-27T18:30:43.930228Z","iopub.status.idle":"2022-11-27T18:30:45.467532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Practica - Twitter\n# ---------------------------------------------------\n# Librerias necesarias para la realizacion de la practica:\n\n# Instalacion:\ninstall.packages(\"rtweet\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"knitr\")\ninstall.packages(\"workcloud\")\ninstall.packages(\"igraph\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"e1071\")\ninstall.packages(\"SparseM\")\ninstall.packages(\"RSelenium\")\ninstall.packages(\"sm\")\ninstall.packages(\"wordcloud\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"hrbrthemes\")\ninstall.packages(\"UpSetR\")\ninstall.packages(\"eechidna\")\ninstall.packages(\"cartogram\")\n\n# Carga:\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(knitr)\nlibrary(workcloud)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(quanteda)\nlibrary(e1071)\nlibrary(SparseM)\nlibrary(RSelenium)\n\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(tm)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(hrbrthemes)\nlibrary(UpSetR)\nlibrary(eechidna)\nlibrary(cartogram)\n\ntema_graf <-\n  theme_minimal() +\n  theme(text = element_text(family = \"serif\"),\n        panel.grid.minor = element_blank(),\n        strip.background = element_rect(fill = \"#EBEBEB\", colour = NA),\n        legend.position = \"none\",\n        legend.box.background = element_rect(fill = \"#EBEBEB\", colour = NA))\n  \n# ---------------------------------------------------\n# Extracción de información en twitter:\n\n# 1) Creación de token con las claves en el API de Twitter\ntoken <- create_token(\napp = \"U5_Practica_Ana\",\nconsumer_key = \"I3vOd8YnGdjCHLixAv9MnrZfg\",\nconsumer_secret = \"AFGGNIqtuMHzH6QBVXPw84UOYxwaQdmIHET6tcbX1YprZvbxYw\",\naccess_token = \"712134627888992256-B6Vbp2bEQ0LjwTWksE6oHXWVxRjLch5\",\naccess_secret = \"X00rSqIdvec8yA0yufnAgf3AVCSsLMgxTOutbF7cIHnc7\"\n)\n\n# 2) Extracción de tweets de tres \n# usuarios mediante la función get_timeline\ndatos_new1 <- get_timeline(user=\"@LuisitoComunica\", n = 200, parse = TRUE, check = TRUE, include_rts = FALSE)\ndatos_new2 <- get_timeline(user=\"@franco_esca\", n = 200, parse = TRUE, check = TRUE, include_rts = FALSE)\ndatos_new3 <- get_timeline(user=\"@MYMALK4PON3\", n = 200, parse = TRUE, check = TRUE, include_rts = FALSE)\n\n# 3) Conversión de columnas numéricas a character, para evitar \n# errores de compatibilidad, mediante la función map_if\ndatos_new1 <- map_if(.x=datos_new1, .p=is.numeric, .f=as.character)\ndatos_new2 <- map_if(.x=datos_new2, .p=is.numeric, .f=as.character)\ndatos_new3 <- map_if(.x=datos_new3, .p=is.numeric, .f=as.character)\n\n# 4) Unión de todos los tweets en un único dataframe\ntweets1 <- bind_rows(datos_new1, datos_new2, datos_new3)\n\n# 5) Mostrar el total de tweets extraídos \nresumen <- tweets1 %>% group_by(\"screen_name\") %>% summarise(numero_tweets = n()); resumen\n\n# 6) Seleccionar  y renombrar columnas\ntweets1 <- tweets1 %>% select(screen_name, created_at, status_id, text)\ntweets1 <- tweets1 %>% rename(autor = screen_name, fecha = created_at, texto = text, tweet_id = status_id)\nhead(tweets1)\ncolnames(tweets1)\n\n# ---------------------------------------------------\n# Limpieza de datos y tokenización:\n\n# crear script  con el siguiente codigo, y guardarlo. \n# El script, para cada tweet:\n# 1) cambia el texto a minúsculas elimina \n# 2) páginas web\n# 3) signos de puntuación\n# 4) números\n# 5) espacios en blanco seguidos\n# 6) separa las palabras con más de 1 caracter en tokens\n\nlimpiar_tokenizar <- function(texto){\nnuevo_texto <- tolower(texto) # 1\n# de cualquier cosa que no sea un espacio)\nnuevo_texto <- str_replace_all(nuevo_texto,\"http\\\\S*\", \"\") # 2\nnuevo_texto <- str_replace_all(nuevo_texto,\"[[:punct:]]\", \" \") # 3\nnuevo_texto <- str_replace_all(nuevo_texto,\"[[:digit:]]\", \" \") # 4\nnuevo_texto <- str_replace_all(nuevo_texto,\"[\\\\s]+\", \" \")  # 5\nnuevo_texto <- str_split(nuevo_texto, \" \")[[1]]   #6\nnuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})\n    return(nuevo_texto)\n}\n\n# 1) Se aplica la función de limpieza y \n# tokenización a cada tweet\ntweets1 <- tweets1 %>% mutate(texto_tokenizado = map(.x = texto, .f = limpiar_tokenizar))\ntweets1 %>% select(texto_tokenizado) %>% head()","metadata":{}},{"cell_type":"code","source":"# Practica - Twitter\n# ---------------------------------------------------\n# Librerias necesarias para la realizacion de la practica:\n\n# Instalacion:\ninstall.packages(\"rtweet\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"knitr\")\n#install.packages(\"workcloud\")\ninstall.packages(\"igraph\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"quanteda\")\ninstall.packages(\"e1071\")\ninstall.packages(\"SparseM\")\ninstall.packages(\"RSelenium\")\ninstall.packages(\"sm\")\ninstall.packages(\"wordcloud\")\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"hrbrthemes\")\ninstall.packages(\"UpSetR\")\ninstall.packages(\"eechidna\")\ninstall.packages(\"cartogram\")\ninstall.packages(\"twitteR\")\ninstall.packages(\"ROAuth\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T18:55:32.720520Z","iopub.execute_input":"2022-11-27T18:55:32.753333Z","iopub.status.idle":"2022-11-27T19:06:08.869702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carga:\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(knitr)\n#library(workcloud)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(quanteda)\nlibrary(e1071)\nlibrary(SparseM)\nlibrary(RSelenium)\n\nlibrary(scales)\nlibrary(gridExtra)\nlibrary(tm)\nlibrary(lubridate)\nlibrary(zoo)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(hrbrthemes)\nlibrary(UpSetR)\nlibrary(eechidna)\nlibrary(cartogram)\nlibrary(twitteR)\nlibrary(ROAuth)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:23:38.517552Z","iopub.execute_input":"2022-11-27T19:23:38.519215Z","iopub.status.idle":"2022-11-27T19:23:42.492432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CONFIGURACIÓN\ntema_graf <-\n  theme_minimal() +\n  theme(text = element_text(family = \"serif\"),\n        panel.grid.minor = element_blank(),\n        strip.background = element_rect(fill = \"#EBEBEB\", colour = NA),\n        legend.position = \"none\",\n        legend.box.background = element_rect(fill = \"#EBEBEB\", colour = NA))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:23:49.162806Z","iopub.execute_input":"2022-11-27T19:23:49.165103Z","iopub.status.idle":"2022-11-27T19:23:49.185002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1) Creación de token con las claves en el API de Twitter\napp = \"BigDataAcevedo\"\nconsumer_key = \"hjB4PfLczGIDn85fWXqeAAAFX\"\nconsumer_secret = \"AjRlhWBC2kgbnVnzjRkZQLlAN8x18BOxKT5ZDEuLIGosB9M0ed\"\naccess_token = \"1388188187433054209-p96F7Jj56cN35NDD1stz6MSVLF8ph9\"\naccess_secret = \"7EcdTVdpL72nRE7JscNOTurYJt2XFgivGpK0BvhJvpWCe\"\n\nsetup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:23:57.268122Z","iopub.execute_input":"2022-11-27T19:23:57.269991Z","iopub.status.idle":"2022-11-27T19:23:57.566508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2) Extracción de tweets de tres \ncuenta1 <- userTimeline(\"@vegetta777\", n = 45)\ncuenta1\ncuenta2 <- userTimeline(\"@Rubiu5\", n=45)\ncuenta2\ncuenta3 <- userTimeline(\"@WillyrexYT\", n=45)\ncuenta3 \ntypeof(cuenta1) \ntypeof(cuenta2)\ntypeof(cuenta3)\n# @Rubiu5 @JeltyTV @ElBadiablo @franco_esca @JuanSGuarnizo\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:32:02.794670Z","iopub.execute_input":"2022-11-27T19:32:02.796186Z","iopub.status.idle":"2022-11-27T19:32:04.244338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3) Conversión de columnas numéricas a character, \n#para evitar errores de compatibilidad, mediante la función map_if\ncuenta1 <- map_if(.x=cuenta1, .p=is.numeric, .f=as.character)\ncuenta1 = twListToDF(cuenta1)\n\ncuenta2 <- map_if(.x=cuenta2, .p=is.numeric, .f=as.character)\ncuenta2 = twListToDF(cuenta2)\n\ncuenta3 <- map_if(.x=cuenta3, .p=is.numeric, .f=as.character)\ncuenta3 = twListToDF(cuenta3)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:32:17.707394Z","iopub.execute_input":"2022-11-27T19:32:17.709050Z","iopub.status.idle":"2022-11-27T19:32:18.267173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4) Unión de todos los tweets en un único dataframe\ntweets1 <- rbind(cuenta1, cuenta2, cuenta3)\ntweets1\nwrite.csv(data, \"Streamers.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:35:38.236749Z","iopub.execute_input":"2022-11-27T19:35:38.239073Z","iopub.status.idle":"2022-11-27T19:35:38.479183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5) Mostrar el total de tweets extraídos \nresumen <- tweets1 %>% group_by(\"screenName\") %>% summarise(numero_tweets = n()); resumen","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:36:13.828178Z","iopub.execute_input":"2022-11-27T19:36:13.829686Z","iopub.status.idle":"2022-11-27T19:36:13.874259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6) Seleccionar  y renombrar columnas\ntweets1 <- tweets1 %>% select(screenName, created, id, text)\ntweets1 <- tweets1 %>% rename(autor = screenName, fecha = created, texto = text, tweet_id = id)\nhead(tweets1)\ncolnames(tweets1)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:49:23.886756Z","iopub.execute_input":"2022-11-27T19:49:23.888579Z","iopub.status.idle":"2022-11-27T19:49:23.969746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#7 Tokeniza - Separa las cadenas en palabras\nlimpiar_tokenizar <- function(texto){\nnuevo_texto <- tolower(texto) # 1\n# de cualquier cosa que no sea un espacio)\nnuevo_texto <- str_replace_all(nuevo_texto,\"http\\\\S*\", \"\") # 2\nnuevo_texto <- str_replace_all(nuevo_texto,\"[[:punct:]]\", \" \") # 3\nnuevo_texto <- str_replace_all(nuevo_texto,\"[[:digit:]]\", \" \") # 4\nnuevo_texto <- str_replace_all(nuevo_texto,\"[\\\\s]+\", \" \")  # 5\n nuevo_texto <- str_replace_all(nuevo_texto,\"[[emoji:]]+\", \" \")   \nnuevo_texto <- str_split(nuevo_texto, \" \")[[1]]   #6\nnuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})\n    return(nuevo_texto)\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:50:51.342308Z","iopub.execute_input":"2022-11-27T19:50:51.343924Z","iopub.status.idle":"2022-11-27T19:50:51.358555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets1 <- tweets1 %>% mutate(texto_tokenizado = map(.x = texto, .f = limpiar_tokenizar))\ntweets1 %>% select(texto_tokenizado) %>% head()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:50:55.018250Z","iopub.execute_input":"2022-11-27T19:50:55.019866Z","iopub.status.idle":"2022-11-27T19:50:55.171745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------- 1) Al tokenizar, el elemento de estudio ha pasado a \n# ser cada token, incumpliendo así la condición de tidy data. \n\ntweets_tidy <- tweets1 %>% select(-texto) %>% unnest()\ntweets_tidy <- tweets_tidy %>% rename(token = texto_tokenizado)\nhead(tweets_tidy) ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:50:59.889817Z","iopub.execute_input":"2022-11-27T19:50:59.891564Z","iopub.status.idle":"2022-11-27T19:50:59.940509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2) Crear una gráfica de la distribución de los tweets \n# recuperados en función de la fecha en que fueron publicados\nggplot(tweets1, aes(x = as.Date(fecha), fill = autor)) +\n  geom_histogram(position = \"identity\", bins = 20, show.legend = FALSE) +\n  scale_x_date(date_labels = \"%m-%Y\", date_breaks = \"2 week\") +\n  labs(x = \"fecha de publicación\", y = \"número de tweets\") +\n  facet_wrap(~ autor, ncol = 1) +  theme_bw() +  theme(axis.text.x = element_text(angle = 90))\n\ntweets_mes_anyo <- tweets1 %>% mutate(mes_anyo = format(fecha, \"%Y-%m\"))\ntweets_mes_anyo %>% group_by(autor, mes_anyo) %>% summarise(n = n()) %>%  ggplot(aes(x = mes_anyo, y = n, color = autor)) +\n  geom_line(aes(group = autor)) +\n  labs(title = \"Número de tweets publicados\", x = \"fecha de publicación\",\n       y = \"número de tweets\") +  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, size = 6),\n        legend.position = \"bottom\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:51:19.618693Z","iopub.execute_input":"2022-11-27T19:51:19.636332Z","iopub.status.idle":"2022-11-27T19:51:20.691504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3) Conteo de palabras por usuario: \ntweets_tidy %>% group_by(autor) %>% summarise(n = n()) \ntweets_tidy %>%  ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:51:31.037976Z","iopub.execute_input":"2022-11-27T19:51:31.040452Z","iopub.status.idle":"2022-11-27T19:51:31.304534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4) onteo de palabras distintas por usuario:\ntweets_tidy %>% select(autor, token) %>% distinct() %>%  group_by(autor) %>%  summarise(palabras_distintas = n()) \ntweets_tidy %>% select(autor, token) %>% distinct() %>%\n            ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T19:51:40.489270Z","iopub.execute_input":"2022-11-27T19:51:40.490916Z","iopub.status.idle":"2022-11-27T19:51:41.208587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8) Relación entre palabras (limpieza de tweets)\nlimpiar <- function(texto){\n# Se convierte todo el texto a minúsculas\n    nuevo_texto <- tolower(texto)\n    # Eliminación de páginas web (palabras que empiezan por \"http.\" seguidas \n    # de cualquier cosa que no sea un espacio)\n    nuevo_texto <- str_replace_all(nuevo_texto,\"http\\\\S*\", \"\")\n    # Eliminación de signos de puntuación\n    nuevo_texto <- str_replace_all(nuevo_texto,\"[[:punct:]]\", \" \")\n    # Eliminación de números\n    nuevo_texto <- str_replace_all(nuevo_texto,\"[[:digit:]]\", \" \")\n    # Eliminación de espacios en blanco múltiples\n    nuevo_texto <- str_replace_all(nuevo_texto,\"[\\\\s]+\", \" \")\n    return(nuevo_texto)\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:21:07.366640Z","iopub.execute_input":"2022-10-30T22:21:07.368238Z","iopub.status.idle":"2022-10-30T22:21:07.382579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 9) Stopwords (palabras que no aportan al texto)\nrequire(sm)\nlista_stopwords <- stopwords(\"spanish\")\n# Se añade el término amp al listado de stopwords\nlista_stopwords <- c(lista_stopwords, \"amp\")\n\n# Se filtran las stopwords\ntweets_tidy <- tweets_tidy %>% filter(!(token %in% lista_stopwords))\nlista_stopwords","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:23:47.841508Z","iopub.execute_input":"2022-10-30T22:23:47.843189Z","iopub.status.idle":"2022-10-30T22:23:47.878224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5) Correlación entre usuarios por palabras utilizadas:\ntweets_spread <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>%\n                 spread(key = autor, value = n, fill = NA, drop = TRUE)\ncor.test(~ Rubiu5 + ElBadiablo, method = \"pearson\", data = tweets_spread)\n\np1 <- ggplot(tweets_spread, aes(Rubiu5, JuanSGuarnizo)) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +      geom_text(aes(label = token), check_overlap = TRUE, vjust = 1.5) + \n      scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) +\n      geom_abline(color = \"red\") + theme_bw() + theme(axis.text.x = element_blank(),\n            axis.text.y = element_blank())\n\np2 <- ggplot(tweets_spread, aes(ElBadiablo, JuanSGuarnizo)) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +      geom_text(aes(label = token), check_overlap = TRUE, vjust = 1.5) + \n      scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) +\n      geom_abline(color = \"red\") + theme_bw() + theme(axis.text.x = element_blank(),\n            axis.text.y = element_blank())\n\ngrid.arrange(p1, p2, nrow = 1)\n# @Rubiu5 @JeltyTV @ElBadiablo @franco_esca @JuanSGuarnizo","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:21:13.793253Z","iopub.execute_input":"2022-10-30T22:21:13.794770Z","iopub.status.idle":"2022-10-30T22:21:14.230637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6) Conteo de palabras comunes entre dos usuarios: \npalabras_comunes <- dplyr::intersect(tweets_tidy %>% filter(autor==\"JuanSGuarnizo\") %>%\n                    select(token), tweets_tidy %>% filter(autor==\"Rubiu5\") %>%\n                    select(token)) %>% nrow()\npaste(\"Número de palabras comunes entre JuanSGuarnizo y Rubiu5: \", palabras_comunes)\n\npalabras_comunes <- dplyr::intersect(tweets_tidy %>% filter(autor==\"JuanSGuarnizo\") %>%\n                    select(token), tweets_tidy %>% filter(autor==\"ElBadiablo\") %>%\n                    select(token)) %>% nrow()\npaste(\"Número de palabras comunes entre JuanSGuarnizo y ElBadiablo: \", palabras_comunes)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:21:19.408772Z","iopub.execute_input":"2022-10-30T22:21:19.410648Z","iopub.status.idle":"2022-10-30T22:21:19.488260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7) Comparación en el uso de palabras:\n# a) Pivotaje y despivotaje\ntweets_spread <- tweets_tidy %>% group_by(autor, token) %>% count(token) %>% spread(key = autor, value = n, fill = 0, drop = TRUE)\ntweets_unpivot <- tweets_spread %>% gather(key = \"autor\", value = \"n\", -token)\n\n# b) Selección de los autores JuanSGuarnizo y Rubiu5\ntweets_unpivot <- tweets_unpivot %>% \nfilter(autor %in% c(\"Rubiu5\", \"JuanSGuarnizo\"))\n\n# c) Se añade el total de palabras de cada autor\ntweets_unpivot <- tweets_unpivot %>% left_join(tweets_tidy %>% group_by(autor) %>%summarise(N = n()), by = \"autor\")\ntweets_unpivot\n\n# d) Cálculo de odds y log of odds de cada palabra\ntweets_logOdds <- tweets_unpivot %>%  mutate(odds = (n + 1) / (N + 1))\ntweets_logOdds <- tweets_logOdds %>% select(autor, token, odds) %>% spread(key = autor, value = odds)\ntweets_logOdds <- tweets_logOdds %>% mutate(log_odds = log(Rubiu5/ElBadiablo), abs_log_odds = abs(log_odds))\ntweets_logOdds <- tweets_logOdds %>% mutate(autor_frecuente = if_else(log_odds > 0, \"@Rubiu5\", \"@ElBadiablo\"))\ntweets_logOdds %>% arrange(desc(abs_log_odds)) %>% head() \na <- tweets_logOdds %>% group_by(autor_frecuente) %>% top_n(15, abs_log_odds)\n\n# f) Graficar resultados\nggplot(a,aes(x = reorder(token, log_odds), y = log_odds, fill = autor_frecuente)) + \ngeom_col() + labs(x = \"palabra\", y = \"log odds ratio (Rubiu5 / ElBadiablo)\") + coord_flip() +  theme_bw()\n\n\n#Representación gráfica de las frecuencias\ntweets_tidy %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%\n                top_n(10, n) %>% arrange(autor, desc(n)) %>%\n                ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +\n                geom_col() +\n                theme_bw() +\n                labs(y = \"\", x = \"\") +\n                theme(legend.position = \"none\") +\n                coord_flip() +\n                facet_wrap(~autor,scales = \"free\", ncol = 1, drop = TRUE)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:42:27.202163Z","iopub.execute_input":"2022-10-30T22:42:27.203681Z","iopub.status.idle":"2022-10-30T22:42:27.524379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Practica 2 - Clasificación de tweets:\n\n# Para poder aplicar algoritmos de clasificación \n# a un texto, es necesario crear una representación \n# numérica del mismo\ntweets_rubi_juan <- tweets1 %>% filter(autor %in% c(\"Rubiu5\", \"JuanSGuarnizo\"))\nset.seed(123)\ntrain <- sample(x = 1:nrow(tweets_rubi_juan), size= 0.8*nrow(tweets_rubi_juan))\ntweets_train <- tweets_rubi_juan[train, ]\ntweets_test  <- tweets_rubi_juan[-train, ]\n# @Rubiu5  @ElBadiablo  @JuanSGuarnizo","metadata":{"execution":{"iopub.status.busy":"2022-10-30T22:57:28.952762Z","iopub.execute_input":"2022-10-30T22:57:28.954291Z","iopub.status.idle":"2022-10-30T22:57:28.987343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Es importante verificar que la proporción de \n# cada grupo es similar en el set de entrenamiento \n# y en el de test.\ntable(tweets_train$autor) / length(tweets_train$autor)\ntable(tweets_test$autor) / length(tweets_test$autor)\n\n# Limpieza y tokenización de los documentos de entrenamiento\ntweets_train$texto <- tweets_train$texto %>% map(.f = limpiar_tokenizar) %>% map(.f = paste, collapse = \" \") %>% unlist()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:00:54.259914Z","iopub.execute_input":"2022-10-30T23:00:54.261626Z","iopub.status.idle":"2022-10-30T23:00:54.303510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creación de la matriz documento-término\nmatriz_tfidf_train <- dfm(x = tweets_train$texto, remove = lista_stopwords)\n\n# Se reduce la dimensión de la matriz eliminando aquellos \n# términos que aparecen en menos de 5 documentos. Con esto se consigue eliminar ruido.\nmatriz_tfidf_train <- dfm_trim(x = matriz_tfidf_train, min_docfreq = 5)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:07:37.656932Z","iopub.execute_input":"2022-10-30T23:07:37.658600Z","iopub.status.idle":"2022-10-30T23:07:37.809226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conversión de los valores de la matriz a tf-idf\nmatriz_tfidf_train <- dfm_tfidf(matriz_tfidf_train, scheme_tf = \"prop\",scheme_df = \"inverse\")\nmatriz_tfidf_train","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:07:40.316679Z","iopub.execute_input":"2022-10-30T23:07:40.318173Z","iopub.status.idle":"2022-10-30T23:07:40.342036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limpieza y tokenización de los documentos de test\ntweets_test$texto <- tweets_test$texto %>% map(.f = limpiar_tokenizar) %>%map(.f = paste, collapse = \" \") %>% unlist()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:11:43.242587Z","iopub.execute_input":"2022-10-30T23:11:43.244293Z","iopub.status.idle":"2022-10-30T23:11:43.262215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identificación de las dimensiones de la matriz de entrenamiento\n# Los objetos dm() son de clase S4, se accede a sus elementos mediante @\ndimensiones_matriz_train <- matriz_tfidf_train@Dimnames$features","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:11:45.756205Z","iopub.execute_input":"2022-10-30T23:11:45.757910Z","iopub.status.idle":"2022-10-30T23:11:45.773063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conversión de vector a diccionario pasando por lista\ndimensiones_matriz_train <- as.list(dimensiones_matriz_train)\nnames(dimensiones_matriz_train) <- unlist(dimensiones_matriz_train)\ndimensiones_matriz_train <- dictionary(dimensiones_matriz_train)\ndimensiones_matriz_train","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:11:48.607164Z","iopub.execute_input":"2022-10-30T23:11:48.608824Z","iopub.status.idle":"2022-10-30T23:11:48.634371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Proyección de los documentos de test\nmatriz_tfidf_test<-dfm(x=tweets_test$texto,dictionary=dimensiones_matriz_train)\nmatriz_tfidf_test<-dfm_tfidf(matriz_tfidf_test, scheme_tf=\"prop\",scheme_df = \"inverse\")\nmatriz_tfidf_test","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:15:40.748012Z","iopub.execute_input":"2022-10-30T23:15:40.750511Z","iopub.status.idle":"2022-10-30T23:15:40.804540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Como modelo de predicción se emplea una Máquina de soporte vectorial (SVM). \n# Este método de aprendizaje estadístico suele dar buenos resultados en clasificación.\n# Ajuste del modelo\nmodelo_svm <- svm(x = matriz_tfidf_train, y = as.factor(tweets_train$autor),\n                  kernel = \"linear\", cost = 1, scale = TRUE, type = \"C-classification\")\nmodelo_svm","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:16:42.874350Z","iopub.execute_input":"2022-10-30T23:16:42.875983Z","iopub.status.idle":"2022-10-30T23:16:42.907416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicciones\npredicciones <- predict(object = modelo_svm, newdata = matriz_tfidf_test)\n\n# Error de predicción\n# Matriz de confusión\ntable(observado = tweets_test$autor, predicho = predicciones)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:19:14.867223Z","iopub.execute_input":"2022-10-30T23:19:14.868736Z","iopub.status.idle":"2022-10-30T23:19:14.890661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fuente: https://rpubs.com/jboscomendoza/analisis_sentimientos_lexico_afinn\ndownload.file(\"https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv\",\n              \"lexico_afinn.en.es.csv\")\nsentimientos <- read.csv(\"lexico_afinn.en.es.csv\", stringsAsFactors = F, fileEncoding = \"latin1\") %>% \n  tbl_df()\nsentimientos","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:25:52.939685Z","iopub.execute_input":"2022-10-30T23:25:52.941654Z","iopub.status.idle":"2022-10-30T23:25:53.408295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1) En este ejercicio se emplea la clasificación \n# positivo/negativo proporcionada por el diccionario bing.\n\nsentimientos <- get_sentiments(lexicon = \"bing\")\n# head(sentimientos)\nsentimientos <- sentimientos %>% mutate(valor = if_else(sentiment == \"negative\", -1, 1))\nsentimientos","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:35:10.368662Z","iopub.execute_input":"2022-10-30T23:35:10.370414Z","iopub.status.idle":"2022-10-30T23:35:10.446361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentimiento promedio de cada tweet\n# Al disponer de los datos en formato tidy \n# (una palabra por fila), mediante un inner join \n# se añade a cada palabra su sentimiento y \n# se filtran automáticamente todas aquellas \n# palabras para las que no hay información disponible.\ntweets_sent <- inner_join.data.frame(x = tweets_tidy, y = sentimientos, by = c(token = \"palabra\")))\ntweets_sent","metadata":{"execution":{"iopub.status.busy":"2022-10-30T23:39:17.621992Z","iopub.execute_input":"2022-10-30T23:39:17.623447Z","iopub.status.idle":"2022-10-30T23:39:17.636750Z"},"trusted":true},"execution_count":null,"outputs":[]}]}